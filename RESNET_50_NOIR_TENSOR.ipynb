{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "obvious-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "retained-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unavailable-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from VBMF import VBMF\n",
    "from torch.autograd import Variable\n",
    "from tensorly.decomposition import parafac, partial_tucker\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchstat import stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "focal-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "worldwide-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = '/home/ubuntu/lab/pytorch/modelos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "textile-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = 'Entire_Model_RESNET_50_NOIR_40_FINAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "weighted-effect",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(_path+_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "loved-sapphire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-webcam",
   "metadata": {},
   "source": [
    "## Ejemplos de tucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dangerous-village",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.layer1[0].modules\n",
    "_layer = getattr(model, 'layer2')\n",
    "_bottleneck = _layer[0]\n",
    "_conv2 = getattr(_bottleneck, 'conv2')\n",
    "_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "resistant-angel",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tucker_decomposition_conv_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d964d3c33252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_new_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtucker_decomposition_conv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_conv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_new_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tucker_decomposition_conv_layer' is not defined"
     ]
    }
   ],
   "source": [
    "_new_layers = tucker_decomposition_conv_layer(_conv2)\n",
    "_new_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "homeless-poetry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.layer1[0].modules\n",
    "_layer = getattr(model, 'layer1')\n",
    "_bottleneck = _layer[0]\n",
    "_conv2 = getattr(_bottleneck, 'conv1')\n",
    "_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "auburn-kelly",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tucker_decomposition_conv_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d964d3c33252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_new_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtucker_decomposition_conv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_conv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_new_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tucker_decomposition_conv_layer' is not defined"
     ]
    }
   ],
   "source": [
    "_new_layers = tucker_decomposition_conv_layer(_conv2)\n",
    "_new_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "metric-intellectual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "bn1.weight \t torch.Size([64])\n",
      "bn1.bias \t torch.Size([64])\n",
      "bn1.running_mean \t torch.Size([64])\n",
      "bn1.running_var \t torch.Size([64])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv1.weight \t torch.Size([64, 64, 1, 1])\n",
      "layer1.0.bn1.weight \t torch.Size([64])\n",
      "layer1.0.bn1.bias \t torch.Size([64])\n",
      "layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "layer1.0.bn1.running_var \t torch.Size([64])\n",
      "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight \t torch.Size([64])\n",
      "layer1.0.bn2.bias \t torch.Size([64])\n",
      "layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "layer1.0.bn2.running_var \t torch.Size([64])\n",
      "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.0.bn3.weight \t torch.Size([256])\n",
      "layer1.0.bn3.bias \t torch.Size([256])\n",
      "layer1.0.bn3.running_mean \t torch.Size([256])\n",
      "layer1.0.bn3.running_var \t torch.Size([256])\n",
      "layer1.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer1.0.downsample.0.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.0.downsample.1.weight \t torch.Size([256])\n",
      "layer1.0.downsample.1.bias \t torch.Size([256])\n",
      "layer1.0.downsample.1.running_mean \t torch.Size([256])\n",
      "layer1.0.downsample.1.running_var \t torch.Size([256])\n",
      "layer1.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
      "layer1.1.bn1.weight \t torch.Size([64])\n",
      "layer1.1.bn1.bias \t torch.Size([64])\n",
      "layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "layer1.1.bn1.running_var \t torch.Size([64])\n",
      "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight \t torch.Size([64])\n",
      "layer1.1.bn2.bias \t torch.Size([64])\n",
      "layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "layer1.1.bn2.running_var \t torch.Size([64])\n",
      "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.1.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.1.bn3.weight \t torch.Size([256])\n",
      "layer1.1.bn3.bias \t torch.Size([256])\n",
      "layer1.1.bn3.running_mean \t torch.Size([256])\n",
      "layer1.1.bn3.running_var \t torch.Size([256])\n",
      "layer1.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv1.weight \t torch.Size([64, 256, 1, 1])\n",
      "layer1.2.bn1.weight \t torch.Size([64])\n",
      "layer1.2.bn1.bias \t torch.Size([64])\n",
      "layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "layer1.2.bn1.running_var \t torch.Size([64])\n",
      "layer1.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.2.bn2.weight \t torch.Size([64])\n",
      "layer1.2.bn2.bias \t torch.Size([64])\n",
      "layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "layer1.2.bn2.running_var \t torch.Size([64])\n",
      "layer1.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer1.2.conv3.weight \t torch.Size([256, 64, 1, 1])\n",
      "layer1.2.bn3.weight \t torch.Size([256])\n",
      "layer1.2.bn3.bias \t torch.Size([256])\n",
      "layer1.2.bn3.running_mean \t torch.Size([256])\n",
      "layer1.2.bn3.running_var \t torch.Size([256])\n",
      "layer1.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv1.weight \t torch.Size([128, 256, 1, 1])\n",
      "layer2.0.bn1.weight \t torch.Size([128])\n",
      "layer2.0.bn1.bias \t torch.Size([128])\n",
      "layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "layer2.0.bn1.running_var \t torch.Size([128])\n",
      "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight \t torch.Size([128])\n",
      "layer2.0.bn2.bias \t torch.Size([128])\n",
      "layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "layer2.0.bn2.running_var \t torch.Size([128])\n",
      "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.0.bn3.weight \t torch.Size([512])\n",
      "layer2.0.bn3.bias \t torch.Size([512])\n",
      "layer2.0.bn3.running_mean \t torch.Size([512])\n",
      "layer2.0.bn3.running_var \t torch.Size([512])\n",
      "layer2.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "layer2.0.downsample.1.weight \t torch.Size([512])\n",
      "layer2.0.downsample.1.bias \t torch.Size([512])\n",
      "layer2.0.downsample.1.running_mean \t torch.Size([512])\n",
      "layer2.0.downsample.1.running_var \t torch.Size([512])\n",
      "layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.1.bn1.weight \t torch.Size([128])\n",
      "layer2.1.bn1.bias \t torch.Size([128])\n",
      "layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "layer2.1.bn1.running_var \t torch.Size([128])\n",
      "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight \t torch.Size([128])\n",
      "layer2.1.bn2.bias \t torch.Size([128])\n",
      "layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "layer2.1.bn2.running_var \t torch.Size([128])\n",
      "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.1.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.1.bn3.weight \t torch.Size([512])\n",
      "layer2.1.bn3.bias \t torch.Size([512])\n",
      "layer2.1.bn3.running_mean \t torch.Size([512])\n",
      "layer2.1.bn3.running_var \t torch.Size([512])\n",
      "layer2.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.2.bn1.weight \t torch.Size([128])\n",
      "layer2.2.bn1.bias \t torch.Size([128])\n",
      "layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "layer2.2.bn1.running_var \t torch.Size([128])\n",
      "layer2.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.2.bn2.weight \t torch.Size([128])\n",
      "layer2.2.bn2.bias \t torch.Size([128])\n",
      "layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "layer2.2.bn2.running_var \t torch.Size([128])\n",
      "layer2.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.2.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.2.bn3.weight \t torch.Size([512])\n",
      "layer2.2.bn3.bias \t torch.Size([512])\n",
      "layer2.2.bn3.running_mean \t torch.Size([512])\n",
      "layer2.2.bn3.running_var \t torch.Size([512])\n",
      "layer2.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv1.weight \t torch.Size([128, 512, 1, 1])\n",
      "layer2.3.bn1.weight \t torch.Size([128])\n",
      "layer2.3.bn1.bias \t torch.Size([128])\n",
      "layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "layer2.3.bn1.running_var \t torch.Size([128])\n",
      "layer2.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.3.bn2.weight \t torch.Size([128])\n",
      "layer2.3.bn2.bias \t torch.Size([128])\n",
      "layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "layer2.3.bn2.running_var \t torch.Size([128])\n",
      "layer2.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer2.3.conv3.weight \t torch.Size([512, 128, 1, 1])\n",
      "layer2.3.bn3.weight \t torch.Size([512])\n",
      "layer2.3.bn3.bias \t torch.Size([512])\n",
      "layer2.3.bn3.running_mean \t torch.Size([512])\n",
      "layer2.3.bn3.running_var \t torch.Size([512])\n",
      "layer2.3.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv1.weight \t torch.Size([256, 512, 1, 1])\n",
      "layer3.0.bn1.weight \t torch.Size([256])\n",
      "layer3.0.bn1.bias \t torch.Size([256])\n",
      "layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "layer3.0.bn1.running_var \t torch.Size([256])\n",
      "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight \t torch.Size([256])\n",
      "layer3.0.bn2.bias \t torch.Size([256])\n",
      "layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "layer3.0.bn2.running_var \t torch.Size([256])\n",
      "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.0.bn3.weight \t torch.Size([1024])\n",
      "layer3.0.bn3.bias \t torch.Size([1024])\n",
      "layer3.0.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.0.bn3.running_var \t torch.Size([1024])\n",
      "layer3.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.0.downsample.0.weight \t torch.Size([1024, 512, 1, 1])\n",
      "layer3.0.downsample.1.weight \t torch.Size([1024])\n",
      "layer3.0.downsample.1.bias \t torch.Size([1024])\n",
      "layer3.0.downsample.1.running_mean \t torch.Size([1024])\n",
      "layer3.0.downsample.1.running_var \t torch.Size([1024])\n",
      "layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.1.bn1.weight \t torch.Size([256])\n",
      "layer3.1.bn1.bias \t torch.Size([256])\n",
      "layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "layer3.1.bn1.running_var \t torch.Size([256])\n",
      "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight \t torch.Size([256])\n",
      "layer3.1.bn2.bias \t torch.Size([256])\n",
      "layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "layer3.1.bn2.running_var \t torch.Size([256])\n",
      "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.1.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.1.bn3.weight \t torch.Size([1024])\n",
      "layer3.1.bn3.bias \t torch.Size([1024])\n",
      "layer3.1.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.1.bn3.running_var \t torch.Size([1024])\n",
      "layer3.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.2.bn1.weight \t torch.Size([256])\n",
      "layer3.2.bn1.bias \t torch.Size([256])\n",
      "layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "layer3.2.bn1.running_var \t torch.Size([256])\n",
      "layer3.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.2.bn2.weight \t torch.Size([256])\n",
      "layer3.2.bn2.bias \t torch.Size([256])\n",
      "layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "layer3.2.bn2.running_var \t torch.Size([256])\n",
      "layer3.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.2.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.2.bn3.weight \t torch.Size([1024])\n",
      "layer3.2.bn3.bias \t torch.Size([1024])\n",
      "layer3.2.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.2.bn3.running_var \t torch.Size([1024])\n",
      "layer3.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.3.bn1.weight \t torch.Size([256])\n",
      "layer3.3.bn1.bias \t torch.Size([256])\n",
      "layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "layer3.3.bn1.running_var \t torch.Size([256])\n",
      "layer3.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.3.bn2.weight \t torch.Size([256])\n",
      "layer3.3.bn2.bias \t torch.Size([256])\n",
      "layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "layer3.3.bn2.running_var \t torch.Size([256])\n",
      "layer3.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.3.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.3.bn3.weight \t torch.Size([1024])\n",
      "layer3.3.bn3.bias \t torch.Size([1024])\n",
      "layer3.3.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.3.bn3.running_var \t torch.Size([1024])\n",
      "layer3.3.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.4.bn1.weight \t torch.Size([256])\n",
      "layer3.4.bn1.bias \t torch.Size([256])\n",
      "layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "layer3.4.bn1.running_var \t torch.Size([256])\n",
      "layer3.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.4.bn2.weight \t torch.Size([256])\n",
      "layer3.4.bn2.bias \t torch.Size([256])\n",
      "layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "layer3.4.bn2.running_var \t torch.Size([256])\n",
      "layer3.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.4.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.4.bn3.weight \t torch.Size([1024])\n",
      "layer3.4.bn3.bias \t torch.Size([1024])\n",
      "layer3.4.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.4.bn3.running_var \t torch.Size([1024])\n",
      "layer3.4.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv1.weight \t torch.Size([256, 1024, 1, 1])\n",
      "layer3.5.bn1.weight \t torch.Size([256])\n",
      "layer3.5.bn1.bias \t torch.Size([256])\n",
      "layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "layer3.5.bn1.running_var \t torch.Size([256])\n",
      "layer3.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.5.bn2.weight \t torch.Size([256])\n",
      "layer3.5.bn2.bias \t torch.Size([256])\n",
      "layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "layer3.5.bn2.running_var \t torch.Size([256])\n",
      "layer3.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer3.5.conv3.weight \t torch.Size([1024, 256, 1, 1])\n",
      "layer3.5.bn3.weight \t torch.Size([1024])\n",
      "layer3.5.bn3.bias \t torch.Size([1024])\n",
      "layer3.5.bn3.running_mean \t torch.Size([1024])\n",
      "layer3.5.bn3.running_var \t torch.Size([1024])\n",
      "layer3.5.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv1.weight \t torch.Size([512, 1024, 1, 1])\n",
      "layer4.0.bn1.weight \t torch.Size([512])\n",
      "layer4.0.bn1.bias \t torch.Size([512])\n",
      "layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "layer4.0.bn1.running_var \t torch.Size([512])\n",
      "layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight \t torch.Size([512])\n",
      "layer4.0.bn2.bias \t torch.Size([512])\n",
      "layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "layer4.0.bn2.running_var \t torch.Size([512])\n",
      "layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.0.bn3.weight \t torch.Size([2048])\n",
      "layer4.0.bn3.bias \t torch.Size([2048])\n",
      "layer4.0.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.0.bn3.running_var \t torch.Size([2048])\n",
      "layer4.0.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.0.downsample.0.weight \t torch.Size([2048, 1024, 1, 1])\n",
      "layer4.0.downsample.1.weight \t torch.Size([2048])\n",
      "layer4.0.downsample.1.bias \t torch.Size([2048])\n",
      "layer4.0.downsample.1.running_mean \t torch.Size([2048])\n",
      "layer4.0.downsample.1.running_var \t torch.Size([2048])\n",
      "layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
      "layer4.1.bn1.weight \t torch.Size([512])\n",
      "layer4.1.bn1.bias \t torch.Size([512])\n",
      "layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "layer4.1.bn1.running_var \t torch.Size([512])\n",
      "layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight \t torch.Size([512])\n",
      "layer4.1.bn2.bias \t torch.Size([512])\n",
      "layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "layer4.1.bn2.running_var \t torch.Size([512])\n",
      "layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.1.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.1.bn3.weight \t torch.Size([2048])\n",
      "layer4.1.bn3.bias \t torch.Size([2048])\n",
      "layer4.1.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.1.bn3.running_var \t torch.Size([2048])\n",
      "layer4.1.bn3.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv1.weight \t torch.Size([512, 2048, 1, 1])\n",
      "layer4.2.bn1.weight \t torch.Size([512])\n",
      "layer4.2.bn1.bias \t torch.Size([512])\n",
      "layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "layer4.2.bn1.running_var \t torch.Size([512])\n",
      "layer4.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.2.bn2.weight \t torch.Size([512])\n",
      "layer4.2.bn2.bias \t torch.Size([512])\n",
      "layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "layer4.2.bn2.running_var \t torch.Size([512])\n",
      "layer4.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "layer4.2.conv3.weight \t torch.Size([2048, 512, 1, 1])\n",
      "layer4.2.bn3.weight \t torch.Size([2048])\n",
      "layer4.2.bn3.bias \t torch.Size([2048])\n",
      "layer4.2.bn3.running_mean \t torch.Size([2048])\n",
      "layer4.2.bn3.running_var \t torch.Size([2048])\n",
      "layer4.2.bn3.num_batches_tracked \t torch.Size([])\n",
      "fc.weight \t torch.Size([4, 2048])\n",
      "fc.bias \t torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "visible-apple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "                 module name   input shape  output shape      params memory(MB)             MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0                      conv1     3 224 224    64 112 112      9408.0       3.06    235,225,088.0    118,013,952.0    639744.0    3211264.0      19.20%    3851008.0\n",
      "1                        bn1    64 112 112    64 112 112       128.0       3.06      3,211,264.0      1,605,632.0   3211776.0    3211264.0       4.37%    6423040.0\n",
      "2                       relu    64 112 112    64 112 112         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       1.15%    6422528.0\n",
      "3                    maxpool    64 112 112    64  56  56         0.0       0.77      1,605,632.0        802,816.0   3211264.0     802816.0       3.60%    4014080.0\n",
      "4             layer1.0.conv1    64  56  56    64  56  56      4096.0       0.77     25,489,408.0     12,845,056.0    819200.0     802816.0       7.25%    1622016.0\n",
      "5               layer1.0.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.23%    1606144.0\n",
      "6             layer1.0.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       0.75%    1753088.0\n",
      "7               layer1.0.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.24%    1606144.0\n",
      "8             layer1.0.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       4.96%    4079616.0\n",
      "9               layer1.0.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.78%    6424576.0\n",
      "10             layer1.0.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.04%    6422528.0\n",
      "11     layer1.0.downsample.0    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.60%    4079616.0\n",
      "12     layer1.0.downsample.1   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.80%    6424576.0\n",
      "13            layer1.1.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       0.63%    4079616.0\n",
      "14              layer1.1.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.06%    1606144.0\n",
      "15            layer1.1.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       0.51%    1753088.0\n",
      "16              layer1.1.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.07%    1606144.0\n",
      "17            layer1.1.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.58%    4079616.0\n",
      "18              layer1.1.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.80%    6424576.0\n",
      "19             layer1.1.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.04%    6422528.0\n",
      "20            layer1.2.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       0.83%    4079616.0\n",
      "21              layer1.2.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.22%    1606144.0\n",
      "22            layer1.2.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       0.58%    1753088.0\n",
      "23              layer1.2.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.23%    1606144.0\n",
      "24            layer1.2.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.58%    4079616.0\n",
      "25              layer1.2.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.82%    6424576.0\n",
      "26             layer1.2.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.03%    6422528.0\n",
      "27            layer2.0.conv1   256  56  56   128  56  56     32768.0       1.53    205,119,488.0    102,760,448.0   3342336.0    1605632.0       1.68%    4947968.0\n",
      "28              layer2.0.bn1   128  56  56   128  56  56       256.0       1.53      1,605,632.0        802,816.0   1606656.0    1605632.0       0.41%    3212288.0\n",
      "29            layer2.0.conv2   128  56  56   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0   2195456.0     401408.0       0.77%    2596864.0\n",
      "30              layer2.0.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.05%     803840.0\n",
      "31            layer2.0.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       0.82%    2269184.0\n",
      "32              layer2.0.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.42%    3215360.0\n",
      "33             layer2.0.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.02%    3211264.0\n",
      "34     layer2.0.downsample.0   256  56  56   512  28  28    131072.0       1.53    205,119,488.0    102,760,448.0   3735552.0    1605632.0       3.48%    5341184.0\n",
      "35     layer2.0.downsample.1   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.33%    3215360.0\n",
      "36            layer2.1.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       0.67%    2269184.0\n",
      "37              layer2.1.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.05%     803840.0\n",
      "38            layer2.1.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.56%    1392640.0\n",
      "39              layer2.1.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.05%     803840.0\n",
      "40            layer2.1.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       0.66%    2269184.0\n",
      "41              layer2.1.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.11%    3215360.0\n",
      "42             layer2.1.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.02%    3211264.0\n",
      "43            layer2.2.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       0.65%    2269184.0\n",
      "44              layer2.2.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.05%     803840.0\n",
      "45            layer2.2.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.44%    1392640.0\n",
      "46              layer2.2.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.05%     803840.0\n",
      "47            layer2.2.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       0.78%    2269184.0\n",
      "48              layer2.2.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.43%    3215360.0\n",
      "49             layer2.2.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.02%    3211264.0\n",
      "50            layer2.3.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       0.75%    2269184.0\n",
      "51              layer2.3.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.12%     803840.0\n",
      "52            layer2.3.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       0.54%    1392640.0\n",
      "53              layer2.3.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.05%     803840.0\n",
      "54            layer2.3.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       0.99%    2269184.0\n",
      "55              layer2.3.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.42%    3215360.0\n",
      "56             layer2.3.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.02%    3211264.0\n",
      "57            layer3.0.conv1   512  28  28   256  28  28    131072.0       0.77    205,320,192.0    102,760,448.0   2129920.0     802816.0       1.40%    2932736.0\n",
      "58              layer3.0.bn1   256  28  28   256  28  28       512.0       0.77        802,816.0        401,408.0    804864.0     802816.0       0.22%    1607680.0\n",
      "59            layer3.0.conv2   256  28  28   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   3162112.0     200704.0       0.88%    3362816.0\n",
      "60              layer3.0.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.05%     403456.0\n",
      "61            layer3.0.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.71%    2052096.0\n",
      "62              layer3.0.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.07%    1613824.0\n",
      "63             layer3.0.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.02%    1605632.0\n",
      "64     layer3.0.downsample.0   512  28  28  1024  14  14    524288.0       0.77    205,320,192.0    102,760,448.0   3702784.0     802816.0       1.05%    4505600.0\n",
      "65     layer3.0.downsample.1  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.09%    1613824.0\n",
      "66            layer3.1.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.71%    2052096.0\n",
      "67              layer3.1.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.04%     403456.0\n",
      "68            layer3.1.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.74%    2760704.0\n",
      "69              layer3.1.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.04%     403456.0\n",
      "70            layer3.1.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.69%    2052096.0\n",
      "71              layer3.1.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.07%    1613824.0\n",
      "72             layer3.1.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.02%    1605632.0\n",
      "73            layer3.2.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.71%    2052096.0\n",
      "74              layer3.2.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.04%     403456.0\n",
      "75            layer3.2.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.63%    2760704.0\n",
      "76              layer3.2.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.04%     403456.0\n",
      "77            layer3.2.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.67%    2052096.0\n",
      "78              layer3.2.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.07%    1613824.0\n",
      "79             layer3.2.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.02%    1605632.0\n",
      "80            layer3.3.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.71%    2052096.0\n",
      "81              layer3.3.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.04%     403456.0\n",
      "82            layer3.3.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.62%    2760704.0\n",
      "83              layer3.3.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.04%     403456.0\n",
      "84            layer3.3.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.67%    2052096.0\n",
      "85              layer3.3.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.07%    1613824.0\n",
      "86             layer3.3.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.02%    1605632.0\n",
      "87            layer3.4.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.71%    2052096.0\n",
      "88              layer3.4.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.04%     403456.0\n",
      "89            layer3.4.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.47%    2760704.0\n",
      "90              layer3.4.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.04%     403456.0\n",
      "91            layer3.4.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.68%    2052096.0\n",
      "92              layer3.4.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.07%    1613824.0\n",
      "93             layer3.4.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.02%    1605632.0\n",
      "94            layer3.5.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.71%    2052096.0\n",
      "95              layer3.5.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.04%     403456.0\n",
      "96            layer3.5.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       0.62%    2760704.0\n",
      "97              layer3.5.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.04%     403456.0\n",
      "98            layer3.5.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.68%    2052096.0\n",
      "99              layer3.5.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.07%    1613824.0\n",
      "100            layer3.5.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.02%    1605632.0\n",
      "101           layer4.0.conv1  1024  14  14   512  14  14    524288.0       0.38    205,420,544.0    102,760,448.0   2899968.0     401408.0       1.37%    3301376.0\n",
      "102             layer4.0.bn1   512  14  14   512  14  14      1024.0       0.38        401,408.0        200,704.0    405504.0     401408.0       0.05%     806912.0\n",
      "103           layer4.0.conv2   512  14  14   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9838592.0     100352.0       1.47%    9938944.0\n",
      "104             layer4.0.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.04%     204800.0\n",
      "105           layer4.0.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.72%    4696064.0\n",
      "106             layer4.0.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.06%     819200.0\n",
      "107            layer4.0.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.02%     802816.0\n",
      "108    layer4.0.downsample.0  1024  14  14  2048   7   7   2097152.0       0.38    205,420,544.0    102,760,448.0   9191424.0     401408.0       1.20%    9592832.0\n",
      "109    layer4.0.downsample.1  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.07%     819200.0\n",
      "110           layer4.1.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       0.93%    4696064.0\n",
      "111             layer4.1.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.04%     204800.0\n",
      "112           layer4.1.conv2   512   7   7   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       1.32%    9637888.0\n",
      "113             layer4.1.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.04%     204800.0\n",
      "114           layer4.1.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       0.88%    4696064.0\n",
      "115             layer4.1.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.07%     819200.0\n",
      "116            layer4.1.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.01%     802816.0\n",
      "117           layer4.2.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       0.89%    4696064.0\n",
      "118             layer4.2.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.05%     204800.0\n",
      "119           layer4.2.conv2   512   7   7   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       0.71%    9637888.0\n",
      "120             layer4.2.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.04%     204800.0\n",
      "121           layer4.2.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       0.91%    4696064.0\n",
      "122             layer4.2.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.07%     819200.0\n",
      "123            layer4.2.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.01%     802816.0\n",
      "124                  avgpool  2048   7   7  2048   1   1         0.0       0.01              0.0              0.0         0.0          0.0       2.36%          0.0\n",
      "125                       fc          2048             4      8196.0       0.00         16,380.0          8,192.0     40976.0         16.0       2.00%      40992.0\n",
      "total                                                     23516228.0     109.68  8,215,558,652.0  4,116,497,408.0     40976.0         16.0     100.00%  324682016.0\n",
      "===================================================================================================================================================================\n",
      "Total params: 23,516,228\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 109.68MB\n",
      "Total MAdd: 8.22GMAdd\n",
      "Total Flops: 4.12GFlops\n",
      "Total MemR+W: 309.64MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stat(model.cpu(),(3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-collectible",
   "metadata": {},
   "source": [
    "## Decompose_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "official-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "_decompose_model = '_TENSOR_Entire_Model_RESNET_50_NOIR_40_FINAL'\n",
    "_path = '/home/ubuntu/lab/pytorch/2.Tensor_Decomposition/modelo_comprimidos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "meaning-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_decomposition_conv_layer(layer, rank):\n",
    "    W = layer.weight.data\n",
    "\n",
    "    last, first, vertical, horizontal = parafac(W, rank=rank, init='random')[1]\n",
    "    \n",
    "    pointwise_s_to_r_layer = nn.Conv2d(in_channels=first.shape[0],\n",
    "                                       out_channels=first.shape[1],\n",
    "                                       kernel_size=1,\n",
    "                                       padding=0,\n",
    "                                       bias=False)\n",
    "\n",
    "    depthwise_r_to_r_layer = nn.Conv2d(in_channels=rank,\n",
    "                                       out_channels=rank,\n",
    "                                       kernel_size=vertical.shape[0],\n",
    "                                       stride=layer.stride,\n",
    "                                       padding=layer.padding,\n",
    "                                       dilation=layer.dilation,\n",
    "                                       groups=rank,\n",
    "                                       bias=False)\n",
    "                                       \n",
    "    pointwise_r_to_t_layer = nn.Conv2d(in_channels=last.shape[1],\n",
    "                                       out_channels=last.shape[0],\n",
    "                                       kernel_size=1,\n",
    "                                       padding=0,\n",
    "                                       bias=True)\n",
    "    \n",
    "    if layer.bias is not None:\n",
    "        pointwise_r_to_t_layer.bias.data = layer.bias.data\n",
    "\n",
    "    sr = first.t_().unsqueeze_(-1).unsqueeze_(-1)\n",
    "    rt = last.unsqueeze_(-1).unsqueeze_(-1)\n",
    "    rr = torch.stack([vertical.narrow(1, i, 1) @ torch.t(horizontal).narrow(0, i, 1) for i in range(rank)]).unsqueeze_(1)\n",
    "\n",
    "    pointwise_s_to_r_layer.weight.data = sr \n",
    "    pointwise_r_to_t_layer.weight.data = rt\n",
    "    depthwise_r_to_r_layer.weight.data = rr\n",
    "\n",
    "    new_layers = [pointwise_s_to_r_layer,\n",
    "                  depthwise_r_to_r_layer, pointwise_r_to_t_layer]\n",
    "    return nn.Sequential(*new_layers)\n",
    "    # return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ranging-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcin para estimar los rangos utilizando VBMF\n",
    "def estimate_ranks(layer):\n",
    "    \"\"\" Unfold the 2 modes of the Tensor the decomposition will \n",
    "    be performed on, and estimates the ranks of the matrices using VBMF \n",
    "    \"\"\"\n",
    "    weights = layer.weight.data.cpu().numpy()\n",
    "    unfold_0 = tl.unfold(weights, 0) \n",
    "    unfold_1 = tl.unfold(weights, 1)\n",
    "    _, diag_0, _, _ = VBMF.EVBMF(unfold_0)\n",
    "    _, diag_1, _, _ = VBMF.EVBMF(unfold_1)\n",
    "    ranks = [diag_0.shape[0], diag_1.shape[1]]\n",
    "    return ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "intermediate-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tucker_decomposition_conv_layer(layer):\n",
    "    \"\"\" Gets a conv layer, \n",
    "        returns a nn.Sequential object with the Tucker decomposition.\n",
    "        The ranks are estimated with a Python implementation of VBMF\n",
    "        https://github.com/CasvandenBogaard/VBMF\n",
    "    \"\"\"\n",
    "\n",
    "    ranks = estimate_ranks(layer)\n",
    "    print(layer, \"VBMF Estimated ranks\", ranks)\n",
    "    #print(layer.weight.data.numpy)\n",
    "\n",
    "    core, [last, first] = \\\n",
    "        partial_tucker(layer.weight.detach().numpy(), \\\n",
    "        #partial_tucker(layer.weight.numpy(), \\\n",
    "            modes=[0, 1], rank=ranks, init='svd')\n",
    "    #print(\"ACA!!!!!\")\n",
    "    # A pointwise convolution that reduces the channels from S to R3\n",
    "    first_layer = torch.nn.Conv2d(in_channels=first.shape[0],\n",
    "                                  out_channels=first.shape[1], \n",
    "                                  kernel_size=1,\n",
    "                                  stride=1, \n",
    "                                  padding=0, \n",
    "                                  dilation=layer.dilation, \n",
    "                                  bias=False)\n",
    "\n",
    "    # A regular 2D convolution layer with R3 input channels \n",
    "    # and R3 output channels\n",
    "    core_layer = torch.nn.Conv2d(in_channels=core.shape[1],\n",
    "                                 out_channels=core.shape[0], \n",
    "                                 kernel_size=layer.kernel_size,\n",
    "                                 stride=layer.stride, \n",
    "                                 padding=layer.padding, \n",
    "                                 dilation=layer.dilation,\n",
    "                                 bias=False)\n",
    "\n",
    "    # A pointwise convolution that increases the channels from R4 to T\n",
    "    last_layer = torch.nn.Conv2d(in_channels=last.shape[1],\n",
    "                                 out_channels=last.shape[0], \n",
    "                                 kernel_size=1, \n",
    "                                 stride=1,\n",
    "                                 padding=0, \n",
    "                                 dilation=layer.dilation, \n",
    "                                 bias=True)\n",
    "\n",
    "    if layer.bias is not None:\n",
    "        last_layer.bias.data = layer.bias.data\n",
    "\n",
    "    #print(type(first))\n",
    "    t_first = torch.from_numpy(first.copy())\n",
    "    t_last = torch.from_numpy(last.copy())\n",
    "    t_core = torch.from_numpy(core.copy())\n",
    "    #t_first = torch.Tensor(first)\n",
    "    #t_first = torch.tensor(first)\n",
    "    #print(type(t_first))\n",
    "    #print(torch.transpose(first, 1, 0).unsqueeze(-1).unsqueeze(-1))\n",
    "    first_layer.weight.data = torch.transpose(t_first, 1, 0).unsqueeze(-1).unsqueeze(-1)\n",
    "    last_layer.weight.data = t_last.unsqueeze(-1).unsqueeze(-1)\n",
    "    core_layer.weight.data = t_core\n",
    "\n",
    "    new_layers = [first_layer, core_layer, last_layer]\n",
    "    return nn.Sequential(*new_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "norwegian-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [36, 37]\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [23, 30]\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [25, 24]\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) VBMF Estimated ranks [53, 50]\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [64, 73]\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [53, 54]\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [43, 45]\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) VBMF Estimated ranks [108, 105]\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [95, 104]\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [89, 94]\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [79, 87]\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [73, 81]\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [74, 82]\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) VBMF Estimated ranks [192, 202]\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [152, 187]\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) VBMF Estimated ranks [252, 268]\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "                 module name   input shape  output shape      params memory(MB)             MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0                      conv1     3 224 224    64 112 112      9408.0       3.06    235,225,088.0    118,013,952.0    639744.0    3211264.0       1.01%    3851008.0\n",
      "1                        bn1    64 112 112    64 112 112       128.0       3.06      3,211,264.0      1,605,632.0   3211776.0    3211264.0       0.44%    6423040.0\n",
      "2                       relu    64 112 112    64 112 112         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.17%    6422528.0\n",
      "3                    maxpool    64 112 112    64  56  56         0.0       0.77      1,605,632.0        802,816.0   3211264.0     802816.0       3.86%    4014080.0\n",
      "4             layer1.0.conv1    64  56  56    64  56  56      4096.0       0.77     25,489,408.0     12,845,056.0    819200.0     802816.0       0.47%    1622016.0\n",
      "5               layer1.0.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.24%    1606144.0\n",
      "6           layer1.0.conv2.0    64  56  56    37  56  56      2368.0       0.44     14,736,064.0      7,426,048.0    812288.0     464128.0       0.36%    1276416.0\n",
      "7           layer1.0.conv2.1    37  56  56    36  56  56     11988.0       0.43     75,075,840.0     37,594,368.0    512080.0     451584.0       6.76%     963664.0\n",
      "8           layer1.0.conv2.2    36  56  56    64  56  56      2368.0       0.77     14,450,688.0      7,426,048.0    461056.0     802816.0       0.42%    1263872.0\n",
      "9               layer1.0.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.25%    1606144.0\n",
      "10            layer1.0.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.27%    4079616.0\n",
      "11              layer1.0.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.52%    6424576.0\n",
      "12             layer1.0.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.14%    6422528.0\n",
      "13     layer1.0.downsample.0    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.26%    4079616.0\n",
      "14     layer1.0.downsample.1   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.54%    6424576.0\n",
      "15            layer1.1.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       1.23%    4079616.0\n",
      "16              layer1.1.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.25%    1606144.0\n",
      "17          layer1.1.conv2.0    64  56  56    30  56  56      1920.0       0.36     11,948,160.0      6,021,120.0    810496.0     376320.0       0.32%    1186816.0\n",
      "18          layer1.1.conv2.1    30  56  56    23  56  56      6210.0       0.28     38,876,992.0     19,474,560.0    401160.0     288512.0       0.65%     689672.0\n",
      "19          layer1.1.conv2.2    23  56  56    64  56  56      1536.0       0.77      9,232,384.0      4,816,896.0    294656.0     802816.0       0.36%    1097472.0\n",
      "20              layer1.1.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.25%    1606144.0\n",
      "21            layer1.1.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.27%    4079616.0\n",
      "22              layer1.1.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.52%    6424576.0\n",
      "23             layer1.1.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.13%    6422528.0\n",
      "24            layer1.2.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       1.33%    4079616.0\n",
      "25              layer1.2.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.24%    1606144.0\n",
      "26          layer1.2.conv2.0    64  56  56    24  56  56      1536.0       0.29      9,558,528.0      4,816,896.0    808960.0     301056.0       0.31%    1110016.0\n",
      "27          layer1.2.conv2.1    24  56  56    25  56  56      5400.0       0.30     33,790,400.0     16,934,400.0    322656.0     313600.0       0.64%     636256.0\n",
      "28          layer1.2.conv2.2    25  56  56    64  56  56      1664.0       0.77     10,035,200.0      5,218,304.0    320256.0     802816.0       0.36%    1123072.0\n",
      "29              layer1.2.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.25%    1606144.0\n",
      "30            layer1.2.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.25%    4079616.0\n",
      "31              layer1.2.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.54%    6424576.0\n",
      "32             layer1.2.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.13%    6422528.0\n",
      "33            layer2.0.conv1   256  56  56   128  56  56     32768.0       1.53    205,119,488.0    102,760,448.0   3342336.0    1605632.0       2.30%    4947968.0\n",
      "34              layer2.0.bn1   128  56  56   128  56  56       256.0       1.53      1,605,632.0        802,816.0   1606656.0    1605632.0       0.34%    3212288.0\n",
      "35          layer2.0.conv2.0   128  56  56    50  56  56      6400.0       0.60     39,984,000.0     20,070,400.0   1631232.0     627200.0       0.61%    2258432.0\n",
      "36          layer2.0.conv2.1    50  56  56    53  28  28     23850.0       0.16     37,355,248.0     18,698,400.0    722600.0     166208.0       0.72%     888808.0\n",
      "37          layer2.0.conv2.2    53  28  28   128  28  28      6912.0       0.38     10,637,312.0      5,419,008.0    193856.0     401408.0       0.32%     595264.0\n",
      "38              layer2.0.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.21%     803840.0\n",
      "39            layer2.0.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.20%    2269184.0\n",
      "40              layer2.0.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.36%    3215360.0\n",
      "41             layer2.0.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.12%    3211264.0\n",
      "42     layer2.0.downsample.0   256  56  56   512  28  28    131072.0       1.53    205,119,488.0    102,760,448.0   3735552.0    1605632.0       1.03%    5341184.0\n",
      "43     layer2.0.downsample.1   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.30%    3215360.0\n",
      "44            layer2.1.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       1.25%    2269184.0\n",
      "45              layer2.1.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.20%     803840.0\n",
      "46          layer2.1.conv2.0   128  28  28    73  28  28      9344.0       0.22     14,594,160.0      7,325,696.0    438784.0     228928.0       0.35%     667712.0\n",
      "47          layer2.1.conv2.1    73  28  28    64  28  28     42048.0       0.19     65,881,088.0     32,965,632.0    397120.0     200704.0       0.69%     597824.0\n",
      "48          layer2.1.conv2.2    64  28  28   128  28  28      8320.0       0.38     12,845,056.0      6,522,880.0    233984.0     401408.0       0.35%     635392.0\n",
      "49              layer2.1.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.19%     803840.0\n",
      "50            layer2.1.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.19%    2269184.0\n",
      "51              layer2.1.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.34%    3215360.0\n",
      "52             layer2.1.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.11%    3211264.0\n",
      "53            layer2.2.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       1.23%    2269184.0\n",
      "54              layer2.2.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.21%     803840.0\n",
      "55          layer2.2.conv2.0   128  28  28    54  28  28      6912.0       0.16     10,795,680.0      5,419,008.0    429056.0     169344.0       0.29%     598400.0\n",
      "56          layer2.2.conv2.1    54  28  28    53  28  28     25758.0       0.16     40,346,992.0     20,194,272.0    272376.0     166208.0       0.65%     438584.0\n",
      "57          layer2.2.conv2.2    53  28  28   128  28  28      6912.0       0.38     10,637,312.0      5,419,008.0    193856.0     401408.0       0.33%     595264.0\n",
      "58              layer2.2.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.20%     803840.0\n",
      "59            layer2.2.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.20%    2269184.0\n",
      "60              layer2.2.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.33%    3215360.0\n",
      "61             layer2.2.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.11%    3211264.0\n",
      "62            layer2.3.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       1.24%    2269184.0\n",
      "63              layer2.3.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.20%     803840.0\n",
      "64          layer2.3.conv2.0   128  28  28    45  28  28      5760.0       0.13      8,996,400.0      4,515,840.0    424448.0     141120.0       0.26%     565568.0\n",
      "65          layer2.3.conv2.1    45  28  28    43  28  28     17415.0       0.13     27,273,008.0     13,653,360.0    210780.0     134848.0       0.62%     345628.0\n",
      "66          layer2.3.conv2.2    43  28  28   128  28  28      5632.0       0.38      8,630,272.0      4,415,488.0    157376.0     401408.0       0.31%     558784.0\n",
      "67              layer2.3.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.20%     803840.0\n",
      "68            layer2.3.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.21%    2269184.0\n",
      "69              layer2.3.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.34%    3215360.0\n",
      "70             layer2.3.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.11%    3211264.0\n",
      "71            layer3.0.conv1   512  28  28   256  28  28    131072.0       0.77    205,320,192.0    102,760,448.0   2129920.0     802816.0       2.23%    2932736.0\n",
      "72              layer3.0.bn1   256  28  28   256  28  28       512.0       0.77        802,816.0        401,408.0    804864.0     802816.0       0.25%    1607680.0\n",
      "73          layer3.0.conv2.0   256  28  28   105  28  28     26880.0       0.31     42,065,520.0     21,073,920.0    910336.0     329280.0       0.66%    1239616.0\n",
      "74          layer3.0.conv2.1   105  28  28   108  14  14    102060.0       0.08     39,986,352.0     20,003,760.0    737520.0      84672.0       0.73%     822192.0\n",
      "75          layer3.0.conv2.2   108  14  14   256  14  14     27904.0       0.19     10,838,016.0      5,469,184.0    196288.0     200704.0       0.31%     396992.0\n",
      "76              layer3.0.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.18%     403456.0\n",
      "77            layer3.0.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       1.31%    2052096.0\n",
      "78              layer3.0.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.26%    1613824.0\n",
      "79             layer3.0.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.11%    1605632.0\n",
      "80     layer3.0.downsample.0   512  28  28  1024  14  14    524288.0       0.77    205,320,192.0    102,760,448.0   3702784.0     802816.0       1.03%    4505600.0\n",
      "81     layer3.0.downsample.1  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.24%    1613824.0\n",
      "82            layer3.1.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       1.35%    2052096.0\n",
      "83              layer3.1.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.18%     403456.0\n",
      "84          layer3.1.conv2.0   256  14  14   104  14  14     26624.0       0.08     10,416,224.0      5,218,304.0    307200.0      81536.0       0.31%     388736.0\n",
      "85          layer3.1.conv2.1   104  14  14    95  14  14     88920.0       0.07     34,838,020.0     17,428,320.0    437216.0      74480.0       1.65%     511696.0\n",
      "86          layer3.1.conv2.2    95  14  14   256  14  14     24576.0       0.19      9,533,440.0      4,816,896.0    172784.0     200704.0       0.30%     373488.0\n",
      "87              layer3.1.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.18%     403456.0\n",
      "88            layer3.1.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       1.28%    2052096.0\n",
      "89              layer3.1.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.23%    1613824.0\n",
      "90             layer3.1.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.11%    1605632.0\n",
      "91            layer3.2.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       1.34%    2052096.0\n",
      "92              layer3.2.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.18%     403456.0\n",
      "93          layer3.2.conv2.0   256  14  14    94  14  14     24064.0       0.07      9,414,664.0      4,716,544.0    296960.0      73696.0       0.30%     370656.0\n",
      "94          layer3.2.conv2.1    94  14  14    89  14  14     75294.0       0.07     29,497,804.0     14,757,624.0    374872.0      69776.0       0.69%     444648.0\n",
      "95          layer3.2.conv2.2    89  14  14   256  14  14     23040.0       0.19      8,931,328.0      4,515,840.0    161936.0     200704.0       0.33%     362640.0\n",
      "96              layer3.2.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.18%     403456.0\n",
      "97            layer3.2.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       1.29%    2052096.0\n",
      "98              layer3.2.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.26%    1613824.0\n",
      "99             layer3.2.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.10%    1605632.0\n",
      "100           layer3.3.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       1.33%    2052096.0\n",
      "101             layer3.3.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.18%     403456.0\n",
      "102         layer3.3.conv2.0   256  14  14    87  14  14     22272.0       0.07      8,713,572.0      4,365,312.0    289792.0      68208.0       0.30%     358000.0\n",
      "103         layer3.3.conv2.1    87  14  14    79  14  14     61857.0       0.06     24,232,460.0     12,123,972.0    315636.0      61936.0       0.62%     377572.0\n",
      "104         layer3.3.conv2.2    79  14  14   256  14  14     20480.0       0.19      7,927,808.0      4,014,080.0    143856.0     200704.0       0.27%     344560.0\n",
      "105             layer3.3.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.17%     403456.0\n",
      "106           layer3.3.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       1.28%    2052096.0\n",
      "107             layer3.3.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.28%    1613824.0\n",
      "108            layer3.3.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.10%    1605632.0\n",
      "109           layer3.4.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       1.32%    2052096.0\n",
      "110             layer3.4.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.17%     403456.0\n",
      "111         layer3.4.conv2.0   256  14  14    81  14  14     20736.0       0.06      8,112,636.0      4,064,256.0    283648.0      63504.0       0.29%     347152.0\n",
      "112         layer3.4.conv2.1    81  14  14    73  14  14     53217.0       0.05     20,846,756.0     10,430,532.0    276372.0      57232.0       0.54%     333604.0\n",
      "113         layer3.4.conv2.2    73  14  14   256  14  14     18944.0       0.19      7,325,696.0      3,713,024.0    133008.0     200704.0       0.26%     333712.0\n",
      "114             layer3.4.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.17%     403456.0\n",
      "115           layer3.4.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       1.27%    2052096.0\n",
      "116             layer3.4.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.25%    1613824.0\n",
      "117            layer3.4.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.13%    1605632.0\n",
      "118           layer3.5.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       1.33%    2052096.0\n",
      "119             layer3.5.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.18%     403456.0\n",
      "120         layer3.5.conv2.0   256  14  14    82  14  14     20992.0       0.06      8,212,792.0      4,114,432.0    284672.0      64288.0       0.29%     348960.0\n",
      "121         layer3.5.conv2.1    82  14  14    74  14  14     54612.0       0.06     21,393,400.0     10,703,952.0    282736.0      58016.0       0.57%     340752.0\n",
      "122         layer3.5.conv2.2    74  14  14   256  14  14     19200.0       0.19      7,426,048.0      3,763,200.0    134816.0     200704.0       0.28%     335520.0\n",
      "123             layer3.5.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.18%     403456.0\n",
      "124           layer3.5.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       1.32%    2052096.0\n",
      "125             layer3.5.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.26%    1613824.0\n",
      "126            layer3.5.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.13%    1605632.0\n",
      "127           layer4.0.conv1  1024  14  14   512  14  14    524288.0       0.38    205,420,544.0    102,760,448.0   2899968.0     401408.0       2.44%    3301376.0\n",
      "128             layer4.0.bn1   512  14  14   512  14  14      1024.0       0.38        401,408.0        200,704.0    405504.0     401408.0       0.22%     806912.0\n",
      "129         layer4.0.conv2.0   512  14  14   202  14  14    103424.0       0.15     40,502,616.0     20,271,104.0    815104.0     158368.0       0.67%     973472.0\n",
      "130         layer4.0.conv2.1   202  14  14   192   7   7    349056.0       0.04     34,198,080.0     17,103,744.0   1554592.0      37632.0       0.75%    1592224.0\n",
      "131         layer4.0.conv2.2   192   7   7   512   7   7     98816.0       0.10      9,633,792.0      4,841,984.0    432896.0     100352.0       0.33%     533248.0\n",
      "132             layer4.0.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.18%     204800.0\n",
      "133           layer4.0.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.65%    4696064.0\n",
      "134             layer4.0.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.22%     819200.0\n",
      "135            layer4.0.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.10%     802816.0\n",
      "136    layer4.0.downsample.0  1024  14  14  2048   7   7   2097152.0       0.38    205,420,544.0    102,760,448.0   9191424.0     401408.0       1.51%    9592832.0\n",
      "137    layer4.0.downsample.1  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.22%     819200.0\n",
      "138           layer4.1.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       1.65%    4696064.0\n",
      "139             layer4.1.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.18%     204800.0\n",
      "140         layer4.1.conv2.0   512   7   7   187   7   7     95744.0       0.03      9,373,749.0      4,691,456.0    483328.0      36652.0       0.37%     519980.0\n",
      "141         layer4.1.conv2.1   187   7   7   152   7   7    255816.0       0.03     25,062,520.0     12,534,984.0   1059916.0      29792.0       0.65%    1089708.0\n",
      "142         layer4.1.conv2.2   152   7   7   512   7   7     78336.0       0.10      7,626,752.0      3,838,464.0    343136.0     100352.0       0.29%     443488.0\n",
      "143             layer4.1.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.17%     204800.0\n",
      "144           layer4.1.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.73%    4696064.0\n",
      "145             layer4.1.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.22%     819200.0\n",
      "146            layer4.1.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.10%     802816.0\n",
      "147           layer4.2.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       1.68%    4696064.0\n",
      "148             layer4.2.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.18%     204800.0\n",
      "149         layer4.2.conv2.0   512   7   7   268   7   7    137216.0       0.05     13,434,036.0      6,723,584.0    649216.0      52528.0       0.40%     701744.0\n",
      "150         layer4.2.conv2.1   268   7   7   252   7   7    607824.0       0.05     59,554,404.0     29,783,376.0   2483824.0      49392.0       1.15%    2533216.0\n",
      "151         layer4.2.conv2.2   252   7   7   512   7   7    129536.0       0.10     12,644,352.0      6,347,264.0    567536.0     100352.0       0.36%     667888.0\n",
      "152             layer4.2.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.18%     204800.0\n",
      "153           layer4.2.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       1.62%    4696064.0\n",
      "154             layer4.2.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.25%     819200.0\n",
      "155            layer4.2.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.10%     802816.0\n",
      "156                  avgpool  2048   7   7  2048   1   1         0.0       0.01              0.0              0.0         0.0          0.0       0.26%          0.0\n",
      "157                       fc          2048             4      8196.0       0.00         16,380.0          8,192.0     40976.0         16.0       0.16%      40992.0\n",
      "total                                                     14966673.0     114.91  5,544,985,985.0  2,782,586,088.0     40976.0         16.0     100.00%  301445292.0\n",
      "===================================================================================================================================================================\n",
      "Total params: 14,966,673\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 114.91MB\n",
      "Total MAdd: 5.54GMAdd\n",
      "Total Flops: 2.78GFlops\n",
      "Total MemR+W: 287.48MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mulfunc = (lambda x,y:x*y)\n",
    "for n, m in model.named_children():\n",
    "    if n == 'avgpool':\n",
    "        break\n",
    "    num_children = sum(1 for i in m.children())\n",
    "\n",
    "    if num_children != 0:\n",
    "        # in a layer of resnet\n",
    "        layer = getattr(model, n)\n",
    "        # decomp every bottleneck\n",
    "        for i in range(num_children):\n",
    "            bottleneck = layer[i]\n",
    "            conv2 = getattr(bottleneck, 'conv2')\n",
    "            new_layers = tucker_decomposition_conv_layer(conv2) \n",
    "            setattr(bottleneck, 'conv2', nn.Sequential(*new_layers))\n",
    "\n",
    "            del conv2\n",
    "            del bottleneck\n",
    "        del layer\n",
    "    torch.save(model, _path+_decompose_model)\n",
    "stat(model.cpu(),(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "legal-circumstances",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/lab/pytorch/2.Tensor_Decomposition'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-strategy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
